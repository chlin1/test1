{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BasicLSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chlin1/test1/blob/master/BasicLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "cyJXShPbuKKj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from rnn_utils import *\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EpKOWHmjxOb5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# What is LSTM?\n",
        "1. It is a special type of RNN, capable of learning long-term dependencies.\n",
        "\n",
        "2. \"Long short-term memory (LSTM) units are units of a recurrent neural network (RNN). An RNN composed of LSTM units is often called an LSTM network. A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell\"\n",
        "\n",
        "3. Long Short Term Memory (LSTM) is a type of deep learning model that is mostly used for analysis of sequential data (time series data prediction).\n",
        "\n",
        "4. There are different application areas that are used: Language model, Neural machine translation, Music generation, Time series prediction, Financial prediction, Robot control, Time series prediction, Speech recognition, Rhythm learning, Music composition, Grammar learning, Handwriting recognition, Human action recognition, Sign Language Translation,Time series anomaly detection, Several prediction tasks in the area of business process management, Prediction in medical care pathways, Semantic parsing, Object Co-segmentation.\n",
        "\n",
        "5. LSTM was proposed in 1997 by Sepp Hochreiter and Jürgen Schmidhuber and improved in 2000 by Felix Gers' team."
      ]
    },
    {
      "metadata": {
        "id": "9Ayd39fRuTDz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
        "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
        "    c_prev -- Memory state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
        "    parameters -- python dictionary containing:\n",
        "                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
        "                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
        "                        Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
        "                        bc --  Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
        "                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        bo --  Bias of the output gate, numpy array of shape (n_a, 1)\n",
        "                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
        "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
        "    Returns:\n",
        "    a_next -- next hidden state, of shape (n_a, m)\n",
        "    c_next -- next memory state, of shape (n_a, m)\n",
        "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
        "    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)\n",
        "    Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilde),\n",
        "          c stands for the memory value\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve parameters from \"parameters\"\n",
        "    Wf = parameters[\"Wf\"]\n",
        "    bf = parameters[\"bf\"]\n",
        "    Wi = parameters[\"Wi\"]\n",
        "    bi = parameters[\"bi\"]\n",
        "    Wc = parameters[\"Wc\"]\n",
        "    bc = parameters[\"bc\"]\n",
        "    Wo = parameters[\"Wo\"]\n",
        "    bo = parameters[\"bo\"]\n",
        "    Wy = parameters[\"Wy\"]\n",
        "    by = parameters[\"by\"]\n",
        "\n",
        "    # Retrieve dimensions from shapes of xt and Wy\n",
        "    n_x, m = xt.shape\n",
        "    n_y, n_a = Wy.shape\n",
        "\n",
        "    # Concatenate a_prev and xt\n",
        "    concat = np.zeros((n_a + n_x, m))\n",
        "    concat[: n_a, :] = a_prev\n",
        "    concat[n_a:, :] = xt\n",
        "\n",
        "    # Compute values for ft, it, cct, c_next, ot, a_next\n",
        "    ft = sigmoid(np.matmul(Wf, concat) + bf)\n",
        "    it = sigmoid(np.matmul(Wi, concat) + bi)\n",
        "    cct = np.tanh(np.matmul(Wc, concat) + bc)\n",
        "    c_next = (ft * c_prev) + (it * cct)\n",
        "    ot = sigmoid(np.matmul(Wo, concat) + bo)\n",
        "    a_next = ot * np.tanh(c_next)\n",
        "\n",
        "    # Compute prediction of the LSTM cell\n",
        "    yt_pred = softmax(np.matmul(Wy, a_next) + by)\n",
        "\n",
        "    # store values needed for backward propagation in cache\n",
        "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)\n",
        "\n",
        "    return a_next, c_next, yt_pred, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aL5C2Z7Guh9U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lstm_cell_backward(da_next, dc_next, cache):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    da_next -- Gradients of next hidden state, of shape (n_a, m)\n",
        "    dc_next -- Gradients of next cell state, of shape (n_a, m)\n",
        "    cache -- cache storing information from the forward pass\n",
        "    Returns:\n",
        "    gradients -- python dictionary containing:\n",
        "                        dxt -- Gradient of input data at time-step t, of shape (n_x, m)\n",
        "                        da_prev -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)\n",
        "                        dc_prev -- Gradient w.r.t. the previous memory state, of shape (n_a, m, T_x)\n",
        "                        dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        dWo -- Gradient w.r.t. the weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)\n",
        "                        dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)\n",
        "                        dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)\n",
        "                        dbo -- Gradient w.r.t. biases of the output gate, of shape (n_a, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve information from \"cache\"\n",
        "    (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) = cache\n",
        "\n",
        "    # Retrieve dimensions from xt's and a_next's shape\n",
        "    n_x, m = xt.shape\n",
        "    n_a, m = a_next.shape\n",
        "\n",
        "    # Compute gates related derivatives\n",
        "    dot = da_next * np.tanh(c_next) * ot * (1 - ot)\n",
        "    dcct = (dc_next * it + ot * (1 - np.square(np.tanh(c_next))) * it * da_next) * (1 - np.square(cct))\n",
        "    dit = (dc_next * cct + ot * (1 - np.square(np.tanh(c_next))) * cct * da_next) * it * (1 - it)\n",
        "    dft = (dc_next * c_prev + ot * (1 - np.square(np.tanh(c_next))) * c_prev * da_next) * ft * (1 - ft)\n",
        "\n",
        "    concat = np.concatenate((a_prev, xt), axis=0)\n",
        "\n",
        "    # Compute parameters related derivatives.\n",
        "    dWf = np.dot(dft, concat.T)\n",
        "    dWi = np.dot(dit, concat.T)\n",
        "    dWc = np.dot(dcct, concat.T)\n",
        "    dWo = np.dot(dot, concat.T)\n",
        "    dbf = np.sum(dft, axis=1, keepdims=True)\n",
        "    dbi = np.sum(dit, axis=1, keepdims=True)\n",
        "    dbc = np.sum(dcct, axis=1, keepdims=True)\n",
        "    dbo = np.sum(dot, axis=1, keepdims=True)\n",
        "\n",
        "    # Compute derivatives w.r.t previous hidden state, previous memory state and input. Use equations (15)-(17). (≈3 lines)\n",
        "    da_prev = np.dot(parameters['Wf'][:, :n_a].T, dft) + np.dot(parameters['Wi'][:, :n_a].T, dit) + np.dot(\n",
        "        parameters['Wc'][:, :n_a].T, dcct) + np.dot(parameters['Wo'][:, :n_a].T, dot)\n",
        "    dc_prev = dc_next * ft + ot * (1 - np.square(np.tanh(c_next))) * ft * da_next\n",
        "    dxt = np.dot(parameters['Wf'][:, n_a:].T, dft) + np.dot(parameters['Wi'][:, n_a:].T, dit) + np.dot(\n",
        "        parameters['Wc'][:, n_a:].T, dcct) + np.dot(parameters['Wo'][:, n_a:].T, dot)\n",
        "\n",
        "    # Save gradients in dictionary\n",
        "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dc_prev\": dc_prev, \"dWf\": dWf, \"dbf\": dbf, \"dWi\": dWi, \"dbi\": dbi,\n",
        "                 \"dWc\": dWc, \"dbc\": dbc, \"dWo\": dWo, \"dbo\": dbo}\n",
        "\n",
        "    return gradients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FmrwWFyxvnQj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lstm_forward(x, a0, parameters):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    x -- Input data for every time-step, of shape (n_x, m, T_x).\n",
        "    a0 -- Initial hidden state, of shape (n_a, m)\n",
        "    parameters -- python dictionary containing:\n",
        "                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
        "                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
        "                        Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
        "                        bc -- Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
        "                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        bo -- Bias of the output gate, numpy array of shape (n_a, 1)\n",
        "                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
        "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
        "    Returns:\n",
        "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
        "    y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
        "    caches -- tuple of values needed for the backward pass, contains (list of all the caches, x)\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize \"caches\", which will track the list of all the caches\n",
        "    caches = []\n",
        "\n",
        "    # Retrieve dimensions from shapes of x and parameters['Wy']\n",
        "    n_x, m, T_x = x.shape\n",
        "    n_y, n_a = parameters[\"Wy\"].shape\n",
        "\n",
        "    # initialize \"a\", \"c\" and \"y\" with zeros\n",
        "    a = np.zeros((n_a, m, T_x))\n",
        "    c = a\n",
        "    y = np.zeros((n_y, m, T_x))\n",
        "\n",
        "    # Initialize a_next and c_next\n",
        "    a_next = a0\n",
        "    c_next = np.zeros(a_next.shape)\n",
        "\n",
        "    # loop over all time-steps\n",
        "    for t in range(T_x):\n",
        "        # Update next hidden state, next memory state, compute the prediction, get the cache\n",
        "        a_next, c_next, yt, cache = lstm_cell_forward(x[:, :, t], a_next, c_next, parameters)\n",
        "        # Save the value of the new \"next\" hidden state in a\n",
        "        a[:, :, t] = a_next\n",
        "        # Save the value of the prediction in y\n",
        "        y[:, :, t] = yt\n",
        "        # Save the value of the next cell state\n",
        "        c[:, :, t] = c_next\n",
        "        # Append the cache into caches\n",
        "        caches.append(cache)\n",
        "\n",
        "    # store values needed for backward propagation in cache\n",
        "    caches = (caches, x)\n",
        "\n",
        "    return a, y, c, caches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MJAgeyzyxHhB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lstm_backward(da, caches):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    da -- Gradients w.r.t the hidden states, numpy-array of shape (n_a, m, T_x)\n",
        "    dc -- Gradients w.r.t the memory states, numpy-array of shape (n_a, m, T_x)\n",
        "    caches -- cache storing information from the forward pass (lstm_forward)\n",
        "    Returns:\n",
        "    gradients -- python dictionary containing:\n",
        "                        dx -- Gradient of inputs, of shape (n_x, m, T_x)\n",
        "                        da0 -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)\n",
        "                        dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        dWo -- Gradient w.r.t. the weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)\n",
        "                        dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)\n",
        "                        dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)\n",
        "                        dbo -- Gradient w.r.t. biases of the save gate, of shape (n_a, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve values from the first cache (t=1) of caches.\n",
        "    (caches, x) = caches\n",
        "    (a1, c1, a0, c0, f1, i1, cc1, o1, x1, parameters) = caches[0]\n",
        "\n",
        "    # Retrieve dimensions from da's and x1's shapes\n",
        "    n_a, m, T_x = da.shape\n",
        "    n_x, m = x1.shape\n",
        "\n",
        "    # initialize the gradients with the right sizes\n",
        "    dx = np.zeros((n_x, m, T_x))\n",
        "    da0 = np.zeros((n_a, m))\n",
        "    da_prevt = np.zeros(da0.shape)\n",
        "    dc_prevt = np.zeros(da0.shape)\n",
        "    dWf = np.zeros((n_a, n_a + n_x))\n",
        "    dWi = np.zeros(dWf.shape)\n",
        "    dWc = np.zeros(dWf.shape)\n",
        "    dWo = np.zeros(dWf.shape)\n",
        "    dbf = np.zeros((n_a, 1))\n",
        "    dbi = np.zeros(dbf.shape)\n",
        "    dbc = np.zeros(dbf.shape)\n",
        "    dbo = np.zeros(dbf.shape)\n",
        "\n",
        "    # loop back over the whole sequence\n",
        "    for t in reversed(range(T_x)):\n",
        "        # Compute all gradients using lstm_cell_backward\n",
        "        gradients = lstm_cell_backward(da[:, :, t], dc_prevt, caches[t])\n",
        "        # Store or add the gradient to the parameters' previous step's gradient\n",
        "        dx[:, :, t] = gradients[\"dxt\"]\n",
        "        dWf += gradients[\"dWf\"]\n",
        "        dWi += gradients[\"dWi\"]\n",
        "        dWc += gradients[\"dWc\"]\n",
        "        dWo += gradients[\"dWo\"]\n",
        "        dbf += gradients[\"dbf\"]\n",
        "        dbi += gradients[\"dbi\"]\n",
        "        dbc += gradients[\"dbc\"]\n",
        "        dbo += gradients[\"dbo\"]\n",
        "    # Set the first activation's gradient to the backpropagated gradient da_prev.\n",
        "    da0 = gradients[\"da_prev\"]\n",
        "\n",
        "    # Store the gradients in a python dictionary\n",
        "    gradients = {\"dx\": dx, \"da0\": da0, \"dWf\": dWf, \"dbf\": dbf, \"dWi\": dWi, \"dbi\": dbi,\n",
        "                 \"dWc\": dWc, \"dbc\": dbc, \"dWo\": dWo, \"dbo\": dbo}\n",
        "\n",
        "    return gradients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mjwAY0fcuSdr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "dxESY0CyuOHQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}